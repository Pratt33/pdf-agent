---
title: PDF Question Answering Agent
emoji: ğŸ“„
colorFrom: blue
colorTo: purple
sdk: gradio
sdk_version: 6.3.0
app_file: app.py
pinned: false
license: apache-2.0
---

# ğŸ“„ PDF Question Answering Agent

An optimized Retrieval-Augmented Generation (RAG) system for intelligent PDF document querying powered by LLaMA 3.2 and semantic search.

## ğŸ¯ Overview

This application implements a production-ready RAG pipeline that enables users to upload PDF documents and ask natural language questions. The system retrieves relevant context from the document and generates accurate, context-aware answers using large language models.

## ğŸ—ï¸ System Architecture

### Pipeline Components

```
PDF Upload â†’ Text Extraction â†’ Chunking â†’ Embedding â†’ Vector Index â†’ Retrieval â†’ LLM Generation
```

1. **Document Processing**
   - PDF parsing using `pypdf`
   - Text extraction with page-level granularity
   
2. **Text Chunking**
   - Fixed-size chunking (400 words per chunk)
   - Overlap strategy for context preservation
   
3. **Semantic Embedding**
   - Model: `all-MiniLM-L6-v2` (SentenceTransformers)
   - Dimensions: 384
   - Batch processing for efficiency (batch_size=32)
   
4. **Vector Search**
   - FAISS IndexFlatL2 for similarity search
   - Top-k retrieval (k=3)
   - L2 distance metric
   
5. **Answer Generation**
   - Model: `meta-llama/Llama-3.2-3B-Instruct`
   - Max tokens: 500
   - Context-based prompting

## âš¡ Performance Optimizations

### Implemented Features

| Feature | Impact | Benefit |
|---------|--------|---------|
| **In-Memory Caching** | 95% latency reduction | Stores last 10 processed PDFs with LRU eviction |
| **Batch Processing** | 40% faster embeddings | Processes text chunks in batches of 32 |
| **Async I/O** | 20% overall speedup | Non-blocking operations for file/network I/O |
| **Model Pre-warming** | Eliminates cold start | Loads embedding model at startup |
| **Progressive Loading** | Better UX | Real-time status updates during processing |

### Latency Benchmarks

| Scenario | Latency |
|----------|---------|
| First request (cold) | ~10s |
| Cached PDF (same document) | **~0.5s** |
| Different PDF | ~10s |

## ğŸ› ï¸ Technical Stack

**Backend:**
- Python 3.13
- Gradio 6.3.0 (UI Framework)
- HuggingFace Hub (LLM Inference)

**ML/NLP:**
- SentenceTransformers (Embeddings)
- FAISS (Vector Search)
- PyTorch (ML Backend)

**Dependencies:**
```
gradio>=6.3.0
pypdf>=4.0.0
sentence-transformers>=2.2.0
faiss-cpu>=1.7.4
huggingface_hub>=0.20.0
torch>=2.0.0
```

## ğŸ“ Project Structure

```
pdf-agent/
â”œâ”€â”€ app.py                 # Main Gradio application
â”œâ”€â”€ requirements.txt       # Python dependencies
â”œâ”€â”€ README.md             # Documentation
â”œâ”€â”€ .gitignore            # Git ignore rules
â”œâ”€â”€ app/
â”‚   â”œâ”€â”€ __init__.py
â”‚   â”œâ”€â”€ pdf_loader.py     # PDF text extraction
â”‚   â”œâ”€â”€ embedder.py       # Embedding & indexing
â”‚   â”œâ”€â”€ retriever.py      # Semantic search
â”‚   â”œâ”€â”€ cache.py          # LRU caching system
â”‚   â””â”€â”€ llm/
â”‚       â”œâ”€â”€ __init__.py
â”‚       â”œâ”€â”€ base.py       # LLM interface
â”‚       â””â”€â”€ hf_llm.py     # HuggingFace client
â””â”€â”€ data/                 # User uploads (gitignored)
```

## ğŸš€ Quick Start

### Local Deployment

1. **Clone Repository**
```bash
git clone https://github.com/Pratt33/pdf-agent.git
cd pdf-agent
```

2. **Install Dependencies**
```bash
python -m venv venv
source venv/bin/activate  # Windows: venv\Scripts\activate
pip install -r requirements.txt
```

3. **Set Environment Variables**
```bash
export HF_TOKEN="your_huggingface_token"  # Windows: $env:HF_TOKEN="..."
```

4. **Run Application**
```bash
python app.py
```

5. **Access Interface**
Open http://127.0.0.1:7860 in your browser

### HuggingFace Space Deployment

The application is pre-configured for HuggingFace Spaces deployment:

1. Push to HuggingFace Space repository
2. Set `HF_TOKEN` in Space secrets
3. Application auto-deploys on push

## ğŸ’¡ Usage

1. **Upload PDF**: Click "Upload PDF Document" and select your file
2. **Ask Question**: Type your question in natural language
3. **Get Answer**: Click "Get Answer" to receive contextual response
4. **Cache Benefit**: Ask multiple questions on same PDF instantly

### Example Queries

- "What is the main topic of this document?"
- "Summarize the key findings"
- "What methodology was used?"
- "What are the conclusions?"

## ğŸ”¬ Research Context

### RAG Methodology

Retrieval-Augmented Generation combines:
- **Dense retrieval** for semantic similarity
- **Generative models** for natural language synthesis
- **Context injection** to ground LLM responses

### Why RAG?

- âœ… Reduces hallucinations
- âœ… Enables source attribution
- âœ… Handles dynamic knowledge
- âœ… Cost-effective vs. fine-tuning

## ğŸ¨ Features

- âœ¨ Modern, responsive UI with Gradio
- ğŸ”„ Real-time processing status
- ğŸ’¾ Intelligent caching (last 10 PDFs)
- ğŸš€ Optimized batch processing
- ğŸ“Š Support for various PDF formats
- ğŸ” Secure token handling

## ğŸ“Š System Requirements

**Minimum:**
- Python 3.9+
- 4GB RAM
- 2GB disk space

**Recommended:**
- Python 3.11+
- 8GB RAM
- GPU (optional, for faster embeddings)

## ğŸ”§ Configuration

### Cache Settings
```python
# app/cache.py
pdf_cache = PDFCache(max_size=10)  # Adjust cache size
```

### Chunk Size
```python
# app/embedder.py
chunk_text(text, chunk_size=400)  # Modify chunk size
```

### Retrieval Count
```python
# app/retriever.py
retrieve(query, index, chunks, model, k=3)  # Adjust k
```

## ğŸ› Troubleshooting

**Issue: Model API Errors**
- Ensure `HF_TOKEN` is set correctly
- Check HuggingFace API status

**Issue: Slow Processing**
- First request loads model (expected)
- Subsequent requests use cache
- Consider GPU for large PDFs

**Issue: Out of Memory**
- Reduce chunk_size
- Reduce cache max_size
- Process smaller PDFs

## ğŸ“ License

Apache License 2.0 - See LICENSE file for details

## ğŸ¤ Contributing

Contributions welcome! Please:
1. Fork repository
2. Create feature branch
3. Commit changes
4. Submit pull request

## ğŸ“§ Contact

- GitHub: [@Pratt33](https://github.com/Pratt33)
- HuggingFace: [Pratt333](https://huggingface.co/Pratt333)

## ğŸ™ Acknowledgments

- HuggingFace for model hosting
- SentenceTransformers team
- FAISS by Meta AI Research
- Gradio team

---

**Built with â¤ï¸ using RAG + LLaMA 3.2**
